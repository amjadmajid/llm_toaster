attn_pdrop: 0.1
batch_size: 162
ckpt_config: config
ckpt_dir: llm_model/checkpoints
ckpt_model: model
device: cuda
dtype: long
embd_pdrop: 0.3
eval_inter: 40
eval_iter: 10
lr: 0.0001
max_iter: 100000
micro_batch_size: 32
n_blocks: 8
n_embd: 768
n_head: 12
n_layer: 12
resid_pdrop: 0.1
seq_len: 1024
tokenized_data_path: data/edu_fineweb10B
tokenizer_type: gpt2
vocab_size: 50304

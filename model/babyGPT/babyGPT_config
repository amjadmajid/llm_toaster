batch_size: 6
ckpt_config: config_tmp
ckpt_dir: model/checkpoints
ckpt_model: model_tmp
current_shard: 43
device: cuda
dropout_rate: 0.2
dtype: long
eval_inter: 200
eval_iter: 10
log_file: log.txt
log_inter: 20
lr: 0.0006
max_iter: 100000
max_loss: 3.0736479312181473
n_batches: 16
n_blocks: 16
n_embd: 768
n_head: 8
seq_len: 1024
tokenizer_type: gpt2
training_duration: 225724.18966698647
training_step: 48827
vocab_size: 50304

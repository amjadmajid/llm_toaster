training:
  batch_size: 8
  n_batches: 16  # Number of accumulated batches before stepping the optimizer
  n_embd: 1024
  n_head: 8
  n_blocks: 16
  seq_len: 1024
  lr: 0.0006
  dropout_rate: 0.2
  log_inter: 20
  eval_inter: 200
  eval_iter: 10
  max_iter: 100_000
  dtype: "long"
  tokenizer_type: "gpt2"
  ckpt: "checkpoints/ckpt"
  ckpt_config: "checkpoints/ckpt_config.yaml"
  current_shard: 0
  training_duration: 0.0
  training_step: 0
  log_file: "logs/log.txt"
  data_dir: "dataspace/tokenized_data" # TODO: check how to merge this and data.tokenized_data
  num_workers: 1

# Configurations used for model inference
inference:
  pretrained_model: "babyGPT_254M_QA.llm"
  pretrained_model_config: "babyGPT_254_QA.yaml"
  generate_max_length: 50  # Default maximum length for text generation

# Configurations used for data download and tokenization 
data:
  dataset_name: 'TIGER-Lab/WebInstructSub' #"HuggingFaceFW/fineweb-edu"
  split_ratio: 0.98
  tokenized_data: '../tokenized_data' #"../../dataspace/fineweb"
  remote_name: "sample-10BT"
  shard_size: 10_000_000  # int(1e7)

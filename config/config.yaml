batch_size: 8
n_batches: 16 # number of accumulated batches, before stepping the optimizer
n_embd: 768
n_head: 12
n_layer: 12
seq_len: 1024
lr: 0.0006
dropout_rate: 0.2
log_inter: 20
eval_inter: 200
eval_iter: 10
max_iter: 100_000
n_blocks: 8
dtype: long
tokenizer_type: "gpt2"
vocab_size: 50304 #50257
ckpt_dir: "model/checkpoints"
ckpt_model: "model_tmp"
ckpt_config: "config_tmp"
current_shard: 0
training_duration: 0
training_step: 1 
# device: "cuda"  # or "mps" or "cpu" or "cuda"
 